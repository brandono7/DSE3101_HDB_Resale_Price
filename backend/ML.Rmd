---
title: "ML model"
author: "Jessica Widyawati"
date: "`r Sys.Date()`"
output: html_document
---

```{r warnings = FALSE}
#load the required libraries
library(tidyverse)
library(randomForest)
library(rpart)
library(VIM)
library(gbm)
library(hdm)
```

### Conduct stratified sampling and loading the train and test sets
```{r}
# Set seed for reproducibility
set.seed(42)

hdb_data = readRDS("processed_data/hdb_resale_prices.RDS") 

# Stratified Sampling by 'year'
strata_cols <- c("year")

# Perform the stratified sampling for training set (20%)
train_data <- hdb_data %>%
  group_by(across(all_of(strata_cols))) %>%
  sample_frac(0.2) %>% # Adjust the fraction as needed for training set
  ungroup()

# Remaining data for test sets
test_data <- anti_join(hdb_data, train_data)

# from our EDA results, since we know that the distribution for the price variable is right skewed, we need to transform the price into log price to normalise

train_ml <- train_data %>% mutate(log_price = log(resale_price)) %>%
  select(-resale_price)

test_ml <- test_data %>% mutate(log_price = log(resale_price)) %>%
  select(-resale_price)
test_ml_y <- test_ml[,"log_price"]
test_ml <- test_ml %>%
  select(-log_price)
```

### Benchmark OLS model (Based on domain knowledge)
```{r}
set.seed(42)

# Based on domain knowledge, we picked a few variables that affect HDB prices
regfit = lm(formula= log_price ~ year + remaining_lease + floor_area_sqm + ave_storey +
           + dist_to_nearest_mrt + dist_to_nearest_primary_schools + dist_cbd,data=train_ml)

# Make predictions on the test data
predictions_ols <- predict(regfit, newdata = test_ml)
 
# Calculate RMSE
rmse_ols <- sqrt(mean((test_ml_y - predictions_ols)^2))
 
# Print RMSE
print(paste("RMSE for benchmark OLS:", rmse_ols))

summary(regfit)

# RMSE for benchmark OLS: 0.13494587838384
```

## Machine Learning Techniques
### Regression Trees: 
```{r}
set.seed(42)

# Create decision tree using regression 
fit <- rpart(log_price ~ .,  
             method = "anova", data = train_ml) 
  
# Plot 
plot(fit, uniform = TRUE, 
          main = "log_price prediction using decision trees")
text(fit, use.n = TRUE, cex = .7) 

# method anova is used for regression 
predictions_rt <- predict(fit, test_ml, method = "anova")

# Calculate RMSE
rmse_rt <- sqrt(mean((test_ml_y - predictions_rt)^2))
 
# Print RMSE
print(paste("RMSE for Regression Trees:", rmse_rt))

# RMSE for Regression Trees: 0.185634606041432
```

### Random Forest
### Tuning
```{r}
set.seed(42)
# Now try the Random forest. To go from bagging to proper random forest, we 
# need to add the option mtry - number of predictors randomly sampled for each tree: 
# Here we set mtry= P/3 to reflect the default choice of P/3 for regression problems. 
ntreev = c(500, 1000)

nset = length(ntreev) #number of cases for tree numbers - will determine number of iterations in the loop

for(i in 1:nset) {
  rffit = randomForest(log_price~.,data=train_ml,ntree=ntreev[i], mtry= floor(136/3))
  predictions_rf <- predict(rffit, test_ml, method = "anova") 
  # Calculate RMSE 
  rmse_rt <- sqrt(mean((test_ml_y - predictions_rf)^2)) 
  print(paste("RMSE for Random Forest","(trees:", ntreev[i],"):", rmse_rt))
}

# RMSE for Random Forest (trees: 500): 0.0641815677018226

# RMSE for Random Forest (trees: 1000 ): 0.0631247324693827

# We pick ntree = 500 due to both faster runtime and lower RMSE.
```

### Model with tuned parameters
```{r}
set.seed(42)
rffit = randomForest(log_price~.,data=train_ml,ntree=500, mtry= floor(136/3)) 

plot(rffit) #plot the OOB error 

# method anova is used for regression 
predictions_rf <- predict(rffit, test_ml, method = "anova") 

# Calculate RMSE 
rmse_rf <- sqrt(mean((test_ml_y - predictions_rf)^2)) 

# Print RMSE 
print(paste("RMSE for Random Forest:", rmse_rf))

# Saving the model
# saveRDS(rffit, file = "RandomForest.rds")

# rffit = readRDS("RandomForest.rds")

# RMSE for Random Forest: 0.0641815677018226
```

### Bagging
```{r}
set.seed(42)
#we use the same n_tree for both rf and bagging
bagfit = randomForest(log_price~.,data=train_ml, ntree=500, mtry=136)

plot(bagfit) #plot the OOB error 

# method anova is used for regression 
predictions_bag <- predict(bagfit, test_ml, method = "anova") 

# Calculate RMSE 
rmse_bag <- sqrt(mean((test_ml_y - predictions_bag)^2)) 

# Print RMSE 
print(paste("RMSE for Bagging:", rmse_bag))

# Saving the model
# saveRDS(bagfit, file = "Bagging.rds")

# bagfit = readRDS("Bagging.rds")
# RMSE for Bagging: 0.0670008408203258
```

# Traditional Gradient Boosting Machine (GBM)
```{r}
set.seed(42)

# Define the parameter grid
ntree_values <- c(500, 1000)  # Number of trees
interaction_depth_values <- c(2, 5)  # Interaction depth

# Initialize variables to store best parameters and performance
best_ntree <- NULL
best_interaction_depth <- NULL
best_performance <- Inf  # Initialize with a large value for minimization problems

# Perform grid search
for (ntree in ntree_values) {
  for (depth in interaction_depth_values) {
    # Train GBM model on training data with current hyperparameters
    gbm_model <- gbm(log_price ~ ., data = train_ml, distribution = 'gaussian',
                     interaction.depth = depth, n.trees = ntree, shrinkage = 0.01, cv.folds = 10)
    
    # Evaluate performance (you can use different metrics here)
    # For example, you might want to use mean squared error from cross-validation

    gbm_perf_plot <- gbm.perf(gbm_model, method = "cv")
    
    pdf("gbm_perf_plot.pdf")
    print(gbm_perf_plot)
    dev.off()

    performance <- gbm.perf(gbm_model, method = "cv")

    print(gbm_perf_plot)

    
    # Manually calculate cross-validation error
    best_tree_index <- which.min(gbm_model$cv.error)  
    
    # Get MSE for best tree
    mse_cv <- gbm_model$cv.error[best_tree_index]  
    
    # Update best parameters if performance is improved
    if (mse_cv < best_performance) {
      best_ntree <- ntree
      best_interaction_depth <- depth
      best_performance <- mse_cv
    }
  }
}

# Print the best hyperparameters and performance
print(paste("Best number of trees:", best_ntree))
print(paste("Best interaction depth:", best_interaction_depth))
print(paste("Best cross-validated MSE:", best_performance))

# best number of trees: 1000, can choose 1000 trees bc for GBM, the runtime for 1000 trees is not painfully long.
# best interaction depth: 5
# best CV MSE = 0.0084000
```

# fit Traditional Gradient Boosting with optimal hyperparameters
```{r}
set.seed(42)
best_interaction_depth = 5
best_ntree = 1000

# Train final GBM model with best hyperparameters on the entire training set
final_gbm_model <- gbm(log_price ~ ., data = train_ml, distribution = 'gaussian',
                        interaction.depth = best_interaction_depth, n.trees = best_ntree,
                        shrinkage = 0.01, cv.folds = 10)
      
# Make predictions on test data
predictions_gbm <- predict(final_gbm_model, (test_ml), n.trees = best_ntree, type = "response")
      
# Calculate RMSE
rmse_gbm <- sqrt(mean((test_ml_y - predictions_gbm)^2))

print(paste("RMSE for Gradient Boosting: ", rmse_gbm))

# Saving the model
# saveRDS(final_gbm_model, file = "GBM.rds")

# final_gbm_model = readRDS("GBM.rds")

# RMSE for Gradient Boosting: 0.0919899399523445
```

## Post-LASSO Regression
```{r}
set.seed(42)

plasso = rlasso(log_price~., data=train_ml,penalty=list(X.dependent.lambda=FALSE, homoscedastic=FALSE),  post=TRUE)

# Make predictions on the test data
predictions_plasso <- predict(plasso, newdata = test_ml)
 
# Calculate RMSE
rmse_plasso <- sqrt(mean((test_ml_y - predictions_plasso)^2))
 
# Print RMSE
print(paste("RMSE for Post Lasso Regression:", rmse_plasso))

# RMSE for Post Lasso Regression: 0.104452066137839
```

### XG Boost 
```{r}
set.seed(42)
library(xgboost) 
library(recipes)
xgb_train <- recipe(log_price ~ ., data = train_ml) %>%
  step_integer(all_nominal()) %>%
  prep(training = train_ml, retain = TRUE) %>%
  juice()

X_train <- as.matrix(xgb_train[setdiff(names(xgb_train), "log_price")])
Y_train <- xgb_train$log_price


xgb.fit <- xgboost(
  data = X_train,
  label = Y_train,
  nrounds = 500,
  objective = "reg:squarederror"
)

X_test <- as.matrix(test_ml)

predictions_xg <- predict(xgb.fit, X_test)

# Calculate RMSE 
rmse_xg <- sqrt(mean((test_ml_y - predictions_xg)^2)) 

# Print RMSE 
print(paste("RMSE for XG Boost:", rmse_xg))
# RMSE for XG Boost: 0.057909164860733

# variable importance plot
# Compute feature importance matrix
importance_matrix = xgb.importance(model = xgb.fit)
importance_matrix
xgb.plot.importance(importance_matrix[1:7,])
features_xg = importance_matrix$Feature[1:7] # we select the top 7 features

formula <- as.formula(paste("log_price ~", paste(features_xg, collapse = " + ")))
xg_ols = lm(formula, data = train_ml)

# Make predictions on the test data
predictions_xgols <- predict(xg_ols, newdata = test_ml)
 
# Calculate RMSE
rmse_xgols <- sqrt(mean((test_ml_y - predictions_xgols)^2))
 
# Print RMSE
print(paste("RMSE for XGBoost OLS:", rmse_xgols))

# RMSE for XGBoost OLS: 0.128174207150794
summary(xg_ols)

# Saving the model
# saveRDS(xgb.fit, file = "xgb.rds")

# xgb.fit = readRDS("xgb.rds")
```

### h step ahead forecast using XGBoost

```{r}
generate_month_dummies <- function() {
  # Get the current year and month
  current_date <- Sys.Date()
  current_year <- as.integer(format(current_date, "%Y"))
  current_month <- as.integer(format(current_date, "%m"))
  
  # Initialize a data frame to store the dummy variables
  dummy_data <- data.frame(dummy_date = as.Date(character()), stringsAsFactors = FALSE)
  year = c()
  month = c()
  
  # Iterate over the next 12 months
  for (i in 0:11) {
    # Calculate the year and month for the current iteration
    next_year <- year + current_year + floor((current_month + i - 1) / 12)
    next_month <- month + ((current_month + i - 1) %% 12) + 1
    
    # Create a dummy variable for the current year and month
    dummy_var <- as.Date(paste(next_year, sprintf("%02d", next_month), "01", sep = "-"))
    
    # Add the dummy variable to the data frame
    col_name <- paste0("year_", next_year, "_month_", sprintf("%02d", next_month))
    dummy_data[, col_name] <- ifelse(dummy_data$dummy_date == dummy_var, 1, 0)
  }
  
  # Remove the dummy_date column
  dummy_data$dummy_date <- NULL
  
  print(month)
}

# Example usage:
month_dummies <- generate_month_dummies()
print(month_dummies)


```

```{r}
xgb_model_optimal = readRDS("xgb.rds")

```
