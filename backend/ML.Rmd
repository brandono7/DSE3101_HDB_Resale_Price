---
title: "ML model"
author: "Jessica Widyawati"
date: "`r Sys.Date()`"
output: html_document
---

```{r warnings = FALSE}
#load the required libraries
library(tidyverse)
library(caret)
```

### Conduct stratified sampling and loading the train and test sets
```{r}
# Set seed for reproducibility
set.seed(42)

hdb_data = read.csv("processed_data/hdb_resale_prices.csv") %>% 
  select(-1)

# Stratified Sampling by 'year'
strata_cols <- c("year")

# Perform the stratified sampling for training set (60%)
train_data <- hdb_data %>%
  group_by(across(all_of(strata_cols))) %>%
  sample_frac(0.6) %>% # Adjust the fraction as needed for training set
  ungroup()

# Remaining data for validation and test sets
remaining_data <- anti_join(hdb_data, train_data)

# Split remaining data into validation and test sets (20% each)
validation_data <- remaining_data %>%
  group_by(across(all_of(strata_cols))) %>%
  sample_frac(0.5) %>% # Adjust the fraction as needed for validation set
  ungroup() 

test_data <- anti_join(remaining_data, validation_data)
```

## Create train,test and validation sets
```{r}
# from our EDA results, since we know that the distribution for the price variable is right skewed, we need to transform the price into log price to normalise

train_ml <- train_data %>% mutate(log_price = log(resale_price)) %>%
  select(-resale_price)

test_ml <- test_data %>% mutate(log_price = log(resale_price)) %>%
  select(-resale_price)

validation_ml <- validation_data %>% mutate(log_price = log(resale_price)) %>%
  select(-resale_price)
```

## Benchmark OLS model
```{r}
# Perform linear regression on all the covariates in the training data
model <- lm(log_price ~ ., data = train_ml)
 
# Make predictions on the test data
predictions <- predict(model, newdata = test_ml)
 
# Calculate RMSE
rmse <- sqrt(mean((test_ml$log_price - predictions)^2))
 
# Print RMSE
print(paste("RMSE for benchmark OLS:", rmse))

summary(model)
```

## Machine Learning Techniques
### Regression Trees: 
```{r}
# Load the package 
library(rpart) 

# Create decision tree using regression 
fit <- rpart(log_price ~ .,  
             method = "anova", data = train_ml) 
  
# Plot 
plot(fit, uniform = TRUE, 
          main = "log_price prediction using decision trees")
text(fit, use.n = TRUE, cex = .7) 

# Print model 
print(fit) 

# method anova is used for regression 
predictions_rt <- predict(fit, test_ml, method = "anova")

# Calculate RMSE
rmse_rt <- sqrt(mean((test_ml$log_price - predictions_rt)^2))
 
# Print RMSE
print(paste("RMSE for Regression Trees:", rmse_rt))
```

### Need changes
```{r}
library(randomForest)
# Random Forest
#Now try the Random forest. To go from bagging to proper random forest, we
#need to add the option mtry - number of predictors randomly sampled for each tree:
#Here we set mtry=4 to reflect the default choice of P/3 for regression problems.
rffit = randomForest(log_price~.,data=train_ml,ntree=5000, mtry=4)

windows()
plot(rffit, ylim=c(13,24)) #plot the OOB error

#Recall bagging is achieved with mtry=P, since here we have 13 predictors, set mtry=13:
rffit2 = randomForest(log_price~.,data=train_ml,ntree=5000,mtry=138)
windows()
plot(rffit2)

#Do the plot of RF vs. bagging:
xx=seq(1,5000,1)
windows()
plot(xx,rffit$mse, type="l",col="blue",ylim=c(13,24), ylab="OOB MSE", xlab="B", main="Bagging vs. Random forest")
lines(xx,rffit2$mse, type="l", col="red")
legend("topright", c("Random Forest, m=4","Bagging"), lty=c(1,1) ,col=c("blue","red"))

#"Poor" performance of RF due to the structure of the data - one really strong predictor, so if it gets kicked out,
#the RF performance really suffers - it pays to understand how methods work to avoid losing performance!
```

### Need changes
```{r}
# Gradient Boosting Machine (GBM)
# Load the gbm package
library(gbm)

# Separate predictors (X) and response variable (Y)
X <- subset(train_ml, select = -c(log_price)) 

Y <- train_ml$log_price

#tune number of trees and interaction depth using 10 fold CV

# Train GBM model
gbm_model <- gbm(Y ~ ., data = X, distribution = "gaussian", n.trees = 100, interaction.depth = 3)

# Summary of the trained model
summary(gbm_model)

# Predictions on training data
predictions_gbm <- predict(gbm_model, X, n.trees = 100, type = "response")

# Evaluate model performance (if needed)
print(gbm_model)

# Calculate RMSE
rmse_gbm <- sqrt(mean((test_ml$log_price - predictions_gbm)^2))
 
# Print RMSE
print(paste("RMSE for Gradient Boosting Model:", rmse_gbm))
```

##XG Boost
```{r}
library(tidymodels)
library(tidyverse)
library(mlbench)

#define pre-processing of data using the recipe package
rec <- recipe(log_price ~ ., data = train_ml)

lasso_mod <- linear_reg(mode = "regression",
                        penalty = tune(),
                        mixture = 1) %>% 
  set_engine("glmnet")

wf <- workflow() %>%
  add_model(lasso_mod) %>%
  add_recipe(rec)
```

### Need changes
## Hedonic price function using post-LASSO and feature engineering
```{r}

```

# Principle Component Analysis
```{r}
# From our EDA, since we know that some variables are highly correlated with each other, we transform the variables into independent variables using PCA

# Preprocessing: Standardize the features
# pca is train and test sets?
scaled_X_train <- scale(train_ml)

# Perform PCA
pca_train <- prcomp(scaled_X_train)

# Extract transformed data
X_pca_train <- pca_train$x

# X_pca now contains your data transformed using PCA
```