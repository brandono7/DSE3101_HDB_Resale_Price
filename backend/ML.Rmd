---
title: "ML model"
author: "Jessica Widyawati"
date: "`r Sys.Date()`"
output: html_document
---

```{r}

#load the required libraries
library(tidyverse)
library(caret)

#read the dataset (the final one)
hdb <- read.csv("backend/processed_data/hdb_resale_prices.csv") %>% select(-1)

```

###load the train n test sets

```{r}

# Set seed for reproducibility
set.seed(42)

hdb_data = read.csv("processed_data/hdb_merged_no_transform.csv") %>% 
  select(-1) %>%
  mutate(month = as.character(month),postal_2dig = as.character(postal_2dig))

# Stratified Sampling by 'year'
strata_cols <- c("year")

# Perform the stratified sampling for training set (60%)
train_data <- hdb_data %>%
  group_by(across(all_of(strata_cols))) %>%
  sample_frac(0.6)%>% # Adjust the fraction as needed for training set
  ungroup()

# Remaining data for validation and test sets
remaining_data <- anti_join(hdb_data, train_data)

# Split remaining data into validation and test sets (20% each)
validation_data <- remaining_data %>%
  group_by(across(all_of(strata_cols))) %>%
  sample_frac(0.5) %>% # Adjust the fraction as needed for validation set
  ungroup() 

test_data <- anti_join(remaining_data, validation_data)

# Reset row names
rownames(train_data) <- NULL
rownames(validation_data) <- NULL
  ungroup()

rownames(test_data) <- NULL

get_categorical_columns <- function(data) {
  categorical_columns <- character(0)  # Initialize an empty character vector to store column names
  for (col in names(data)) {
    if (is.factor(data[[col]]) || is.character(data[[col]])) {
      categorical_columns <- c(categorical_columns, col)
    }
  }
  return(categorical_columns)
}

one_hot_encoding <- function(data, column_names) {
  for (column_name in column_names) {
    # Get unique values in the specified column
    unique_values <- unique(data[[column_name]])
    # Create new columns for each unique value and populate with binary values
    for (value in unique_values) {
      binary_column <- as.integer(data[[column_name]] == value)
      new_column_name <- paste(column_name, value, sep = "_")
      data <- cbind(data, binary_column)
      names(data)[ncol(data)] <- new_column_name
    }
    # Remove the original column
    data <- data[, !names(data) %in% column_name]
  }
  return(data)
}

cat_columns <- hdb_data %>% get_categorical_columns()
train_data = one_hot_encoding(train_data, cat_columns)
validation_data = one_hot_encoding(validation_data, cat_columns)
test_data = one_hot_encoding(test_data, cat_columns)

```

```{r}
#from our EDA results, since we know that the distribution for the price variable is skewed, we need to transform the price into log price

train_ml <- train_data %>% mutate(log_price = log(resale_price)) %>%
  select(-resale_price)

test_ml <- test_data %>% mutate(log_price = log(resale_price)) %>%
  select(-resale_price)

validation_ml <- validation_data %>% mutate(log_price = log(resale_price)) %>%
  select(-resale_price)

#from our EDA, since we know that the variables are highly correlated with each other, we transform the variables into independent variables using PCA

# Add PCA
# Assuming X is your dataset with a large number of variables

# Preprocessing: Standardize the features
# pca is train and test sets?

scaled_X_train <- scale(train_ml)

# Perform PCA
pca_train <- prcomp(scaled_X_train)

# Extract transformed data
X_pca_train <- pca_train$x

# X_pca now contains your data transformed using PCA
```

## benchmark OLS model
```{r}
# Assuming 'train_data' and 'test_data' are your training and test datasets
 
# Perform linear regression on all covariates in the training data
model <- lm(log_price ~ ., data = train_ml)
 
# Make predictions on the test data
predictions <- exp(predict(model, newdata = test_ml))
 
# Calculate RMSE
rmse <- sqrt(mean((exp(test_ml$log_price) - predictions)^2))
 
# Print RMSE
print(paste("RMSE for benchmark OLS:", rmse))

```

## hedonic price function using post-LASSO and feature engineering
```{r}
# Load necessary libraries
library(glmnet)
library(hdm)


# Step 1: Load the HDB dataset
train_ml

# Step 2: Feature engineering and preprocessing
# You may need to preprocess your data here, handle missing values, scale variables, etc.
# For demonstration, let's assume you have already preprocessed the data and stored it in a variable named 'X'

# Step 3: Use post-lasso to select relevant features
# Assuming 'log_price' is the outcome variable and 'X' contains all predictor variables
post_lasso <- postlasso(log_price ~ ., data = train_ml)

# Extract selected features
selected_features <- colnames(post_lasso$coefficients[!is.na(postlasso$coefficients)])

# Step 4: Fit a hedonic regression model using the selected features
model <- lm(log_price ~ ., data = train_ml[, c('log_price', selected_features)])

# Step 5: Evaluate the model
summary(model)
```

# regression trees:
# justify why we dont consider regression trees
```{r}
# Random Forest
rf_model <- train(
  resale_price ~ ., 
  data = train_data, 
  method = "rf", 
  trControl = trainControl(method = "cv", number = 10), 
  tuneLength = 10)

print(rf_model)
```


```{r}
# Gradient Boosting Machine (GBM)
gbm_model <- train(
  log_price ~., 
  data = train_data, 
  method = "gbm", 
  trControl = trainControl(method = "cv", number = 10), 
  tuneLength = 10)

print(gbm_model)
```

### regression decision tree using another code
```{r}
# Load the package 
library(rpart) 

# Create decision tree using regression 
fit <- rpart(log_price ~ .,  
             method = "anova", data = train_ml) 
  
# Plot 
plot(fit, uniform = TRUE, 
          main = "log_price prediction using decision trees")
text(fit, use.n = TRUE, cex = .7) 

# Print model 
print(fit) 

  
# Predicting sepal width 
# using testing data and model 
# method anova is used for regression 
cat("Predicted value:\n") 
predictions_rt <- exp(predict(fit, test_ml, method = "anova")) 

# Calculate RMSE
rmse_rt <- sqrt(mean((exp(test_ml$log_price) - predictions_rt)^2))
 
# Print RMSE
print(paste("RMSE for Regression Trees:", rmse_rt))

```