---
title: "ML model"
author: "Jessica Widyawati"
date: "`r Sys.Date()`"
output: html_document
---

```{r warnings = FALSE}
#load the required libraries
library(tidyverse)
library(caret)
```

### Conduct stratified sampling and loading the train and test sets
```{r}
# Set seed for reproducibility
set.seed(42)

hdb_data = read.csv("processed_data/hdb_resale_prices.csv") %>% 
  select(-1)

# Stratified Sampling by 'year'
strata_cols <- c("year")

# Perform the stratified sampling for training set (60%)
train_data <- hdb_data %>%
  group_by(across(all_of(strata_cols))) %>%
  sample_frac(0.6) %>% # Adjust the fraction as needed for training set
  ungroup()

# Remaining data for validation and test sets
remaining_data <- anti_join(hdb_data, train_data)

# Split remaining data into validation and test sets (20% each)
validation_data <- remaining_data %>%
  group_by(across(all_of(strata_cols))) %>%
  sample_frac(0.5) %>% # Adjust the fraction as needed for validation set
  ungroup() 

test_data <- anti_join(remaining_data, validation_data)

# from our EDA results, since we know that the distribution for the price variable is right skewed, we need to transform the price into log price to normalise

train_ml <- train_data %>% mutate(log_price = log(resale_price)) %>%
  select(-resale_price)

test_ml <- test_data %>% mutate(log_price = log(resale_price)) %>%
  select(-resale_price)

validation_ml <- validation_data %>% mutate(log_price = log(resale_price)) %>%
  select(-resale_price)
```

## Benchmark OLS model
```{r}
# Perform linear regression on all the covariates in the training data
model <- lm(log_price ~ ., data = train_ml)
 
# Make predictions on the test data
predictions <- predict(model, newdata = test_ml)
 
# Calculate RMSE
rmse_ols <- sqrt(mean((test_ml$log_price - predictions)^2))
 
# Print RMSE
print(paste("RMSE for benchmark OLS:", rmse_ols))

summary(model)
```

## Machine Learning Techniques
### Regression Trees: 
```{r}
# Load the package 
library(rpart) 

# Create decision tree using regression 
fit <- rpart(log_price ~ .,  
             method = "anova", data = train_ml) 
  
# Plot 
plot(fit, uniform = TRUE, 
          main = "log_price prediction using decision trees")
text(fit, use.n = TRUE, cex = .7) 

# method anova is used for regression 
predictions_rt <- predict(fit, test_ml, method = "anova")

# Calculate RMSE
rmse_rt <- sqrt(mean((test_ml$log_price - predictions_rt)^2))
 
# Print RMSE
print(paste("RMSE for Regression Trees:", rmse_rt))
```

### Random Forest
### Tuning
```{r}
library(randomForest)
# Now try the Random forest. To go from bagging to proper random forest, we 
# need to add the option mtry - number of predictors randomly sampled for each tree: 
# Here we set mtry= P/3 to reflect the default choice of P/3 for regression problems. 
ntreev = c(30,40,50)

nset = length(ntreev) #number of cases for tree numbers - will determine number of iterations in the loop

for(i in 1:nset) {
  rffit = randomForest(log_price~.,data=validation_ml,ntree=ntreev[i], mtry= 138/3)
  predictions_rf <- predict(rffit, test_ml, method = "anova") 
  # Calculate RMSE 
  rmse_rt <- sqrt(mean((test_ml$log_price - predictions_rf)^2)) 
  print(paste("RMSE for Random Forest:","(trees:", ntreev[i],")", rmse_rt))
}
# We pick ntree = 50 as our tuning parameter given the lowest RMSE
```
### (alternative code for tuning with 10 fold CV)
```{r}
library(randomForest)

# Define the number of trees to tune over
ntreev <- c(30, 40, 50)

# Number of folds for cross-validation
nfolds <- 10

# Initialize vector to store RMSE values
rmse_values <- numeric(length(ntreev))

# Loop over each number of trees
for (i in seq_along(ntreev)) {
  # Initialize vector to store RMSE values for this number of trees
  rmse_values_cv <- numeric(nfolds)
  
  # Perform cross-validation
  for (fold in 1:nfolds) {
    # Split data into training and validation sets for this fold
    fold_size <- nrow(validation_ml) / nfolds
    fold_start <- (fold - 1) * fold_size + 1
    fold_end <- fold * fold_size
    
    # Ensure indices are integers
    validation_indices <- as.integer(fold_start:fold_end)
    train_indices <- setdiff(1:nrow(validation_ml), validation_indices)
    
    # Train Random Forest model
    rffit <- tryCatch(
      randomForest(log_price ~ ., data = validation_ml[train_indices, ], ntree = ntreev[i], mtry = 138/3),
      error = function(e) {
        print(paste("Error in training Random Forest for fold", fold, "and ntree", ntreev[i], ":"))
        print(e)
        return(NULL)
      }
    )
    
    if (is.null(rffit)) next  # Skip to the next fold if training failed
    
    # Make predictions on validation set
    predictions_rf <- predict(rffit, validation_ml[validation_indices, ], method = "anova")
    
    # Calculate RMSE
    rmse_values_cv[fold] <- sqrt(mean((validation_ml$log_price[validation_indices] - predictions_rf)^2))
  }
  
  # Take the average RMSE across all folds
  rmse_values[i] <- mean(rmse_values_cv)
  
  # Print RMSE for this number of trees
  print(paste("Average RMSE for Random Forest (trees:", ntreev[i], "):", rmse_values[i]))
}

# Select the number of trees with the lowest RMSE
best_ntree <- ntreev[which.min(rmse_values)]
print(paste("Best number of trees:", best_ntree))


```

### Model with tuned parameters
```{r}
rffit = randomForest(log_price~.,data=train_ml,ntree=50, mtry= 138/3) 

plot(rffit) #plot the OOB error 

# method anova is used for regression 
predictions_rf <- predict(rffit, test_ml, method = "anova") 

# Calculate RMSE 
rmse_rt <- sqrt(mean((test_ml$log_price - predictions_rf)^2)) 

# Print RMSE 
print(paste("RMSE for Random Forest:", rmse_rt))
```

### XG Boost 
### Tuning (This code is taking too long to run)
```{r}
library(xgboost) 
library(recipes)
xgb_prep <- recipe(log_price ~ ., data = validation_ml) %>%
  step_integer(all_nominal()) %>%
  prep(training = validation_ml, retain = TRUE) %>%
  juice()

X_validation <- as.matrix(xgb_prep[setdiff(names(xgb_prep), "log_price")])
Y_validation <- xgb_prep$log_price

# hyperparameter grid
hyper_grid <- expand.grid(
  eta = 0.01,
  max_depth = 3, 
  min_child_weight = 3,
  subsample = 0.5, 
  colsample_bytree = 0.5,
  gamma = c(0, 1, 10, 100),
  lambda = c(0, 1e-2, 0.1, 1, 100),
  alpha = c(0, 1e-2, 0.1, 1, 100),
  rmse = 0,          # a place to dump RMSE results
  trees = 0          # a place to dump required number of trees
)

# grid search
for(i in seq_len(nrow(hyper_grid))) {
  set.seed(42)
  m <- xgb.cv(
    data = X_validation,
    label = Y_validation,
    nrounds = 100,
    objective = "reg:squarederror",
    early_stopping_rounds = 50, 
    nfold = 10,
    verbose = 0,
    params = list( 
      eta = hyper_grid$eta[i], 
      max_depth = hyper_grid$max_depth[i],
      min_child_weight = hyper_grid$min_child_weight[i],
      subsample = hyper_grid$subsample[i],
      colsample_bytree = hyper_grid$colsample_bytree[i],
      gamma = hyper_grid$gamma[i], 
      lambda = hyper_grid$lambda[i], 
      alpha = hyper_grid$alpha[i]
    ) 
  )
  hyper_grid$rmse[i] <- min(m$evaluation_log$test_rmse_mean)
  hyper_grid$trees[i] <- m$best_iteration
}

# results
hyper_grid %>%
  filter(rmse > 0) %>%
  arrange(rmse) %>%
  glimpse()
```


### Model with tuned parameters
```{r}
xgb_train <- recipe(log_price ~ ., data = train_ml) %>%
  step_integer(all_nominal()) %>%
  prep(training = train_ml, retain = TRUE) %>%
  juice()

X_train <- as.matrix(xgb_train[setdiff(names(xgb_train), "log_price")])
Y_train <- xgb_train$log_price


xgb.fit <- xgboost(
  data = X_train,
  label = Y_train,
  nrounds = 200,
  objective = "reg:squarederror"
)

xgb_test <- recipe(log_price ~ ., data = test_ml) %>%
  step_integer(all_nominal()) %>%
  prep(training = train_ml, retain = TRUE) %>%
  juice()
X_test <- as.matrix(xgb_test[setdiff(names(xgb_test), "log_price")])

predictions_xg <- predict(xgb.fit, X_test)

# Calculate RMSE 
rmse_xg <- sqrt(mean((test_ml$log_price - predictions_xg)^2)) 

# Print RMSE 
print(paste("RMSE for Random Forest:", rmse_xg))

# variable importance plot
vip::vip(xgb.fit) 
```

### Need changes
# Gradient Boosting Machine (GBM)
```{r}
# Load the gbm package
library(gbm)

# Separate predictors (X) and response variable (Y) for validation set
X_val <- subset(validation_ml, select = -c(log_price))
Y_val <- validation_ml$log_price

# Define the number of trees and interaction depth to tune over
ntree_values <- c(50, 100, 150)  # Number of trees
interaction_depth_values <- c(1, 3, 5)  # Interaction depth

# Number of folds for cross-validation
nfolds <- 10

# Initialize vector to store RMSE values
rmse_values <- matrix(NA, nrow = length(ntree_values), ncol = length(interaction_depth_values),
                      dimnames = list(ntree_values, interaction_depth_values))

# Split validation set into folds
validation_folds <- createFolds(1:nrow(validation_ml), k = nfolds)

# Loop over each combination of hyperparameters
for (ntree in ntree_values) {
  for (depth in interaction_depth_values) {
    # Initialize vector to store RMSE values for this combination of hyperparameters
    rmse_values_cv <- numeric(nfolds)
    
    # Perform cross-validation
    for (fold in 1:nfolds) {
      # Get training and validation indices for this fold
      train_indices <- unlist(validation_folds[-fold])
      validation_indices <- validation_folds[[fold]]
      
      # Train GBM model on validation data
      gbm_model <- gbm(Y_val ~ ., data = X_val[train_indices, ], distribution = "gaussian",
                       n.trees = ntree, interaction.depth = depth)
      
      # Make predictions on validation set
      predictions_gbm <- predict(gbm_model, X_val[validation_indices, ], n.trees = ntree, type = "response")
      
      # Calculate RMSE
      rmse_values_cv[fold] <- sqrt(mean((Y_val[validation_indices] - predictions_gbm)^2))
    }
    
    # Take the average RMSE across all folds
    rmse_values[as.character(ntree), as.character(depth)] <- mean(rmse_values_cv)
  }
}

# Find the combination of hyperparameters with the lowest RMSE
min_rmse_gbm <- min(rmse_values, na.rm = TRUE)
best_ntree_gbm <- as.numeric(rownames(which(rmse_values == min_rmse, arr.ind = TRUE)))
best_depth_gbm <- as.numeric(colnames(which(rmse_values == min_rmse, arr.ind = TRUE)))

# Print the best hyperparameters and corresponding RMSE
cat("Best number of trees for Gradient Boosting:", best_ntree, "\n")
cat("Best interaction depth for Gradient Boosting:", best_depth, "\n")
cat("Minimum RMSE for Gradient Boosting:", min_rmse, "\n")

```

# fit Gradient Boosting with optimal hyperparameters
```{r}

# Train GBM model on train data
gbm_model_final <- gbm(Y ~ ., data = X, distribution = "gaussian",
                       n.trees = best_ntree_gbm, interaction.depth = best_depth_gbm)
      
# Make predictions on test data
predictions_gbm <- predict(gbm_model, (test_ml%>%select(-log_price)), n.trees = best_ntree_gbm, type = "response")
      
# Calculate RMSE
rmse_gbm <- sqrt(mean((test_ml$log_price - predictions_gbm)^2))

```


## Hedonic price function using post-LASSO and feature engineering

# ML model
```{r}
# Load required packages
library(glmnet)


# Separate predictors (X) and response variable (Y)
X <- as.matrix(subset(train_ml, select = -c(log_price)))
Y <- train_ml$log_price

# create X and Y for tuning hyperparameter using 10-fold CV
X_val <- as.matrix(subset(validation_ml, select = -c(log_price)))
Y_val <- validation_ml$log_price

# Tune complexity parameter lambda with ten-fold cross-validation
lambda_values <- 10^seq(-1, 1, by = 0.1)  # Range of lambda values to tune over

# Initialize variables to store the best lambda and its corresponding performance
best_lambda <- 0
best_performance <- Inf  # Initialize with a very high value for minimization problems

# Iterate over lambda values
for (lambda in lambda_values) {
  # Fit Lasso regression model
  glmnet_model <- glmnet(X, Y, alpha = 1, lambda = lambda, standardize = TRUE)
  
  # Perform ten-fold cross-validation
  cv_fit <- cv.glmnet(X_val, Y_val, alpha = 1, lambda = lambda_values, standardize = TRUE, nfolds = 10)
  
  # Extract mean squared error (or other performance metric) from cross-validation results
  mean_mse <- min(cv_fit$cvm)
  
  # Check if the current performance is better than the best performance so far
  if (mean_mse < best_performance) {
    best_performance <- mean_mse
    best_lambda <- lambda
  }
}

# Print the best lambda
print(best_lambda)

# Extract optimal lambda
optimal_lambda <- best_lambda

```

# Optimal Post Lasso using optimal lambda
```{r}

# Fit LASSO regression model
lasso_model <- glmnet(x = as.matrix(X), y = Y, alpha = 1, lambda = optimal_lambda, standardize = TRUE)

# Extract selected variables with non-zero coefficients
feature_names <- coef(lasso_model)

selected_vars <- feature_names@Dimnames[[1]][-1]

data = data.frame(Y, train_ml%>%select(selected_vars))

# Fit OLS regression model using selected variables
plasso <- lm(Y ~ ., data = data)

# Make predictions on the test data
predictions_plasso <- predict(plasso, newdata = test_ml)
 
# Calculate RMSE
rmse_plasso <- sqrt(mean((test_ml$log_price - predictions_plasso)^2))
 
# Print RMSE
print(paste("RMSE for hedonic Post Lasso OLS:", rmse_plasso))

#model summary
summary(plasso)
```

# Artificial Neural Network (ANN)
```{r}
ann_model <- train(
  log_price ~ ., 
  data = train_ml, 
  method = "nnet", 
  trControl = trainControl(method = "cv", number = 10), 
  tuneLength = 10,
  trace = FALSE # Set to TRUE for verbose output
)
```

# Principal Component Analysis
```{r}
# From our EDA, since we know that some variables are highly correlated with each other, we transform the variables into independent variables using PCA

# Preprocessing: Standardize the features
# pca is train and test sets?
scaled_X_train <- scale(train_ml)

# Perform PCA
pca_train <- prcomp(scaled_X_train)

# Extract transformed data
X_pca_train <- pca_train$x

# X_pca now contains your data transformed using PCA
```