---
title: "ML model"
author: "Jessica Widyawati"
date: "`r Sys.Date()`"
output: html_document
---

```{r warnings = FALSE}
#load the required libraries
library(tidyverse)
library(caret)
```

### Conduct stratified sampling and loading the train and test sets
```{r}
# Set seed for reproducibility
set.seed(42)

hdb_data = read.csv("processed_data/hdb_merged_no_transform.csv") %>% 
  select(-1) %>%
  mutate(month = as.character(month),postal_2dig = as.character(postal_2dig))

# Stratified Sampling by 'year'
strata_cols <- c("year")

# Perform the stratified sampling for training set (60%)
train_data <- hdb_data %>%
  group_by(across(all_of(strata_cols))) %>%
  sample_frac(0.6) %>% # Adjust the fraction as needed for training set
  ungroup()

# Remaining data for validation and test sets
remaining_data <- anti_join(hdb_data, train_data)

# Split remaining data into validation and test sets (20% each)
validation_data <- remaining_data %>%
  group_by(across(all_of(strata_cols))) %>%
  sample_frac(0.5) %>% # Adjust the fraction as needed for validation set
  ungroup() 

test_data <- anti_join(remaining_data, validation_data)

get_categorical_columns <- function(data) {
  categorical_columns <- character(0)  # Initialize an empty character vector to store column names
  for (col in names(data)) {
    if (is.factor(data[[col]]) || is.character(data[[col]])) {
      categorical_columns <- c(categorical_columns, col)
    }
  }
  return(categorical_columns)
}

one_hot_encoding <- function(data, column_names) {
  for (column_name in column_names) {
    # Get unique values in the specified column
    unique_values <- unique(data[[column_name]])
    # Create new columns for each unique value and populate with binary values
    for (value in unique_values) {
      binary_column <- as.integer(data[[column_name]] == value)
      new_column_name <- paste(column_name, value, sep = "_")
      data <- cbind(data, binary_column)
      names(data)[ncol(data)] <- new_column_name
    }
    # Remove the original column
    data <- data[, !names(data) %in% column_name]
  }
  return(data)
}

cat_columns <- hdb_data %>% get_categorical_columns()
train_data = one_hot_encoding(train_data, cat_columns)
validation_data = one_hot_encoding(validation_data, cat_columns)
test_data = one_hot_encoding(test_data, cat_columns)

as.tibble(hdb_data)
```
## Create train,test and validation sets
```{r}
# from our EDA results, since we know that the distribution for the price variable is right skewed, we need to transform the price into log price to normalise

train_ml <- train_data %>% mutate(log_price = log(resale_price)) %>%
  select(-resale_price)

test_ml <- test_data %>% mutate(log_price = log(resale_price)) %>%
  select(-resale_price)

validation_ml <- validation_data %>% mutate(log_price = log(resale_price)) %>%
  select(-resale_price)
```

## Benchmark OLS model
```{r}
# Perform linear regression on all the covariates in the training data
model <- lm(log_price ~ ., data = train_ml)
 
# Make predictions on the test data
predictions <- exp(predict(model, newdata = test_ml))
 
# Calculate RMSE
rmse <- sqrt(mean((exp(test_ml$log_price) - predictions)^2))
 
# Print RMSE
print(paste("RMSE for benchmark OLS:", rmse))
```

## Machine Learning Techniques
### Regression Trees: 
```{r}
# Load the package 
library(rpart) 

# Create decision tree using regression 
fit <- rpart(log_price ~ .,  
             method = "anova", data = train_ml) 
  
# Plot 
plot(fit, uniform = TRUE, 
          main = "log_price prediction using decision trees")
text(fit, use.n = TRUE, cex = .7) 

# Print model 
print(fit) 

# method anova is used for regression 
predictions_rt <- predict(fit, test_ml, method = "anova")

# Calculate RMSE
rmse_rt <- sqrt(mean((test_ml$log_price - predictions_rt)^2))
 
# Print RMSE
print(paste("RMSE for Regression Trees:", rmse_rt))
```

### Need changes
```{r}
# Random Forest
rf_model <- train(
  resale_price ~ ., 
  data = train_data, 
  method = "rf", 
  trControl = trainControl(method = "cv", number = 10), 
  tuneLength = 10)

print(rf_model)
```

### Need changes
```{r}
# Gradient Boosting Machine (GBM)
# Load the gbm package
library(gbm)

# Separate predictors (X) and response variable (Y)
X <- subset(train_ml, select = -c(log_price)) 

Y <- train_ml$log_price

#tune number of trees and interaction depth using 10 fold CV

# Train GBM model
gbm_model <- gbm(Y ~ ., data = X, distribution = "gaussian", n.trees = 100, interaction.depth = 3)

# Summary of the trained model
summary(gbm_model)

# Predictions on training data
predictions_gbm <- predict(gbm_model, X, n.trees = 100, type = "response")

# Evaluate model performance (if needed)
print(gbm_model)

# Calculate RMSE
rmse_gbm <- sqrt(mean((test_ml$log_price - predictions_gbm)^2))
 
# Print RMSE
print(paste("RMSE for Gradient Boosting Model:", rmse_gbm))
```

##XG Boost
```{r}
library(tidymodels)
library(tidyverse)
library(mlbench)

#define pre-processing of data using the recipe package
rec <- recipe(log_price ~ ., data = train_ml)

lasso_mod <- linear_reg(mode = "regression",
                        penalty = tune(),
                        mixture = 1) %>% 
  set_engine("glmnet")

wf <- workflow() %>%
  add_model(lasso_mod) %>%
  add_recipe(rec)


```

### Need changes
## Hedonic price function using post-LASSO and feature engineering
```{r}

```

# Principle Component Analysis
```{r}
# From our EDA, since we know that some variables are highly correlated with each other, we transform the variables into independent variables using PCA

# Preprocessing: Standardize the features
# pca is train and test sets?
scaled_X_train <- scale(train_ml)

# Perform PCA
pca_train <- prcomp(scaled_X_train)

# Extract transformed data
X_pca_train <- pca_train$x

# X_pca now contains your data transformed using PCA
```