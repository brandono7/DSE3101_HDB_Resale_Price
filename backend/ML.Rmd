---
title: "ML model"
author: "Jessica Widyawati"
date: "`r Sys.Date()`"
output: html_document
---

```{r warnings = FALSE}
#load the required libraries
library(tidyverse)
library(caret)
```

### Conduct stratified sampling and loading the train and test sets
```{r}
# Set seed for reproducibility
set.seed(42)

hdb_data = read.csv("processed_data/hdb_resale_prices.csv") %>% 
  select(-1)

# Stratified Sampling by 'year'
strata_cols <- c("year")

# Perform the stratified sampling for training set (60%)
train_data <- hdb_data %>%
  group_by(across(all_of(strata_cols))) %>%
  sample_frac(0.6) %>% # Adjust the fraction as needed for training set
  ungroup()

# Remaining data for validation and test sets
remaining_data <- anti_join(hdb_data, train_data)

# Split remaining data into validation and test sets (20% each)
validation_data <- remaining_data %>%
  group_by(across(all_of(strata_cols))) %>%
  sample_frac(0.5) %>% # Adjust the fraction as needed for validation set
  ungroup() 

test_data <- anti_join(remaining_data, validation_data)

# from our EDA results, since we know that the distribution for the price variable is right skewed, we need to transform the price into log price to normalise

train_ml <- train_data %>% mutate(log_price = log(resale_price)) %>%
  select(-resale_price)

test_ml <- test_data %>% mutate(log_price = log(resale_price)) %>%
  select(-resale_price)

validation_ml <- validation_data %>% mutate(log_price = log(resale_price)) %>%
  select(-resale_price)
```

## Benchmark OLS model (Based on domain knowledge)
```{r}
# Based on domain knowledge, we picked a few variables that affect HDB prices
regfit = lm(formula= log_price ~ year + remaining_lease + floor_area_sqm + ave_storey +
           + dist_to_nearest_mrt + dist_to_nearest_primary_schools + dist_cbd,data=train_ml)

# Make predictions on the test data
predictions_ols <- predict(regfit, newdata = test_ml)
 
# Calculate RMSE
rmse_ols <- sqrt(mean((test_ml$log_price - predictions_ols)^2))
 
# Print RMSE
print(paste("RMSE for benchmark OLS:", rmse_ols))

summary(regfit)
```
## OLS on all variables
```{r}
# Perform linear regression on all the covariates in the training data
model <- lm(log_price ~ ., data = train_ml)
 
# Make predictions on the test data
predictions <- predict(model, newdata = test_ml)
 
# Calculate RMSE
rmse_ols <- sqrt(mean((test_ml$log_price - predictions)^2))
 
# Print RMSE
print(paste("RMSE for benchmark OLS:", rmse_ols))

summary(model)
```

## Machine Learning Techniques
### Regression Trees: 
```{r}
# Load the package 
library(rpart) 

# Create decision tree using regression 
fit <- rpart(log_price ~ .,  
             method = "anova", data = train_ml) 
  
# Plot 
plot(fit, uniform = TRUE, 
          main = "log_price prediction using decision trees")
text(fit, use.n = TRUE, cex = .7) 

# method anova is used for regression 
predictions_rt <- predict(fit, test_ml, method = "anova")

# Calculate RMSE
rmse_rt <- sqrt(mean((test_ml$log_price - predictions_rt)^2))
 
# Print RMSE
print(paste("RMSE for Regression Trees:", rmse_rt))
```

### Random Forest
### Tuning
```{r}
library(randomForest)
# Now try the Random forest. To go from bagging to proper random forest, we 
# need to add the option mtry - number of predictors randomly sampled for each tree: 
# Here we set mtry= P/3 to reflect the default choice of P/3 for regression problems. 
ntreev = c(500, 1000)

nset = length(ntreev) #number of cases for tree numbers - will determine number of iterations in the loop

for(i in 1:nset) {
  rffit = randomForest(log_price~.,data=validation_ml,ntree=ntreev[i], mtry= 138/3)
  predictions_rf <- predict(rffit, test_ml, method = "anova") 
  # Calculate RMSE 
  rmse_rt <- sqrt(mean((test_ml$log_price - predictions_rf)^2)) 
  print(paste("RMSE for Random Forest:","(trees:", ntreev[i],")", rmse_rt))
}
# We pick ntree = 1000 as our tuning parameter given the lowest RMSE, but take note that the improvement in RMSE is negligible (only 0.0001 improvement), so may choose n = 500 due to runtime concerns. 
```

## may need to delete this
### (alternative code for tuning with 10 fold CV)
```{r}
library(randomForest)

# Define the number of trees to tune over
ntreev <- c(30, 40, 50)

# Number of folds for cross-validation
nfolds <- 10

# Initialize vector to store RMSE values
rmse_values <- numeric(length(ntreev))

# Loop over each number of trees
for (i in seq_along(ntreev)) {
  # Initialize vector to store RMSE values for this number of trees
  rmse_values_cv <- numeric(nfolds)
  
  # Perform cross-validation
  for (fold in 1:nfolds) {
    # Split data into training and validation sets for this fold
    fold_size <- nrow(validation_ml) / nfolds
    fold_start <- (fold - 1) * fold_size + 1
    fold_end <- fold * fold_size
    
    # Ensure indices are integers
    validation_indices <- as.integer(fold_start:fold_end)
    train_indices <- setdiff(1:nrow(validation_ml), validation_indices)
    
    # Train Random Forest model
    rffit <- tryCatch(
      randomForest(log_price ~ ., data = validation_ml[train_indices, ], ntree = ntreev[i], mtry = 138/3),
      error = function(e) {
        print(paste("Error in training Random Forest for fold", fold, "and ntree", ntreev[i], ":"))
        print(e)
        return(NULL)
      }
    )
    
    if (is.null(rffit)) next  # Skip to the next fold if training failed
    
    # Make predictions on validation set
    predictions_rf <- predict(rffit, validation_ml[validation_indices, ], method = "anova")
    
    # Calculate RMSE
    rmse_values_cv[fold] <- sqrt(mean((validation_ml$log_price[validation_indices] - predictions_rf)^2))
  }
  
  # Take the average RMSE across all folds
  rmse_values[i] <- mean(rmse_values_cv)
  
  # Print RMSE for this number of trees
  print(paste("Average RMSE for Random Forest (trees:", ntreev[i], "):", rmse_values[i]))
}

# Select the number of trees with the lowest RMSE
best_ntree <- ntreev[which.min(rmse_values)]
print(paste("Best number of trees:", best_ntree))
```

### Model with tuned parameters
```{r}
rffit = randomForest(log_price~.,data=train_ml,ntree=500, mtry= 138/3) 

plot(rffit) #plot the OOB error 

# method anova is used for regression 
predictions_rf <- predict(rffit, test_ml, method = "anova") 

# Calculate RMSE 
rmse_rt <- sqrt(mean((test_ml$log_price - predictions_rf)^2)) 

# Print RMSE 
print(paste("RMSE for Random Forest:", rmse_rt))
```

### Bagging
```{r}
bagfit = randomForest(log_price~.,data=train_ml,ntree=5000,maxnodes=10,mtry=138)

plot(bagfit) #plot the OOB error 

# method anova is used for regression 
predictions_bag <- predict(bagfit, test_ml, method = "anova") 

# Calculate RMSE 
rmse_bag <- sqrt(mean((test_ml$log_price - predictions_bag)^2)) 

# Print RMSE 
print(paste("RMSE for Random Forest:", rmse_rt))
```


# Traditional Gradient Boosting Machine (GBM)
```{r}
# Load the gbm package
library(gbm)

# Define the parameter grid
ntree_values <- c(500, 1000)  # Number of trees
interaction_depth_values <- c(2, 5)  # Interaction depth

# Initialize variables to store best parameters and performance
best_ntree <- NULL
best_interaction_depth <- NULL
best_performance <- Inf  # Initialize with a large value for minimization problems

# Perform grid search
for (ntree in ntree_values) {
  for (depth in interaction_depth_values) {
    # Train GBM model on training data with current hyperparameters
    gbm_model <- gbm(log_price ~ ., data = train_ml, distribution = 'gaussian',
                     interaction.depth = depth, n.trees = ntree, shrinkage = 0.01, cv.folds = 10)
    
    # Evaluate performance (you can use different metrics here)
    # For example, you might want to use mean squared error from cross-validation
    performance <- gbm.perf(gbm_model, method = "cv")
    mse_cv <- performance$cv.error[gbm_model$best.t]
    
    # Update best parameters if performance is improved
    if (mse_cv < best_performance) {
      best_ntree <- ntree
      best_interaction_depth <- depth
      best_performance <- mse_cv
    }
  }
}


# Print the best hyperparameters and performance
cat("Best number of trees:", best_ntree, "\n")
cat("Best interaction depth:", best_interaction_depth, "\n")
cat("Best cross-validated MSE:", best_performance, "\n")


```
# fit Traditional Gradient Boosting with optimal hyperparameters
```{r}

# Train final GBM model with best hyperparameters on the entire training set
final_gbm_model <- gbm(log_price ~ ., data = train_ml, distribution = 'gaussian',
                        interaction.depth = best_interaction_depth, n.trees = best_ntree,
                        shrinkage = 0.01, cv.folds = 10)
      
# Make predictions on test data
predictions_gbm <- predict(final_gbm_model, (test_ml%>%select(-log_price)), n.trees = best_ntree_gbm, type = "response")
      
# Calculate RMSE
rmse_gbm <- sqrt(mean((test_ml$log_price - predictions_gbm)^2))

print(paste("RMSE for Gradient Boosting: ", rmse_gbm))

```


## Hedonic price function using post-LASSO and feature engineering
# ML model
#suggested by Denis
```{r}
library(hdm)

plasso = rlasso(y~., data=train_ml,penalty=list(X.dependent.lambda=FALSE, homoscedastic=FALSE),  post=TRUE)

# Make predictions on the test data
predictions_plasso <- predict(plasso, newdata = test_ml)
 
# Calculate RMSE
rmse_plasso <- sqrt(mean((test_ml$log_price - predictions_plasso)^2))
 
# Print RMSE
print(paste("RMSE for hedonic Post Lasso Regression:", rmse_plasso))
```


# Artificial Neural Network (ANN)
```{r}
ann_model <- train(
  log_price ~ ., 
  data = train_ml, 
  method = "nnet", 
  trControl = trainControl(method = "cv", number = 10), 
  tuneLength = 10,
  trace = FALSE # Set to TRUE for verbose output
)
```

### XG Boost (may need to delete/change if Denis doesnt like this)
### Tuning (This code is taking too long to run)
```{r}
library(xgboost) 
library(recipes)
xgb_prep <- recipe(log_price ~ ., data = validation_ml) %>%
  step_integer(all_nominal()) %>%
  prep(training = validation_ml, retain = TRUE) %>%
  juice()

X_validation <- as.matrix(xgb_prep[setdiff(names(xgb_prep), "log_price")])
Y_validation <- xgb_prep$log_price

# hyperparameter grid
hyper_grid <- expand.grid(
  eta = 0.01,
  max_depth = 3, 
  min_child_weight = 3,
  subsample = 0.5, 
  colsample_bytree = 0.5,
  gamma = c(0, 1, 10, 100),
  lambda = c(0, 1e-2, 0.1, 1, 100),
  alpha = c(0, 1e-2, 0.1, 1, 100),
  rmse = 0,          # a place to dump RMSE results
  trees = 0          # a place to dump required number of trees
)

# grid search
for(i in seq_len(nrow(hyper_grid))) {
  set.seed(42)
  m <- xgb.cv(
    data = X_validation,
    label = Y_validation,
    nrounds = 100,
    objective = "reg:squarederror",
    early_stopping_rounds = 50, 
    nfold = 10,
    verbose = 0,
    params = list( 
      eta = hyper_grid$eta[i], 
      max_depth = hyper_grid$max_depth[i],
      min_child_weight = hyper_grid$min_child_weight[i],
      subsample = hyper_grid$subsample[i],
      colsample_bytree = hyper_grid$colsample_bytree[i],
      gamma = hyper_grid$gamma[i], 
      lambda = hyper_grid$lambda[i], 
      alpha = hyper_grid$alpha[i]
    ) 
  )
  hyper_grid$rmse[i] <- min(m$evaluation_log$test_rmse_mean)
  hyper_grid$trees[i] <- m$best_iteration
}

# results
hyper_grid %>%
  filter(rmse > 0) %>%
  arrange(rmse) %>%
  glimpse()
```

### Model with tuned parameters
```{r}
library(xgboost) 
library(recipes)
xgb_train <- recipe(log_price ~ ., data = train_ml) %>%
  step_integer(all_nominal()) %>%
  prep(training = train_ml, retain = TRUE) %>%
  juice()

X_train <- as.matrix(xgb_train[setdiff(names(xgb_train), "log_price")])
Y_train <- xgb_train$log_price


xgb.fit <- xgboost(
  data = X_train,
  label = Y_train,
  nrounds = 500,
  objective = "reg:squarederror"
)

xgb_test <- recipe(log_price ~ ., data = test_ml) %>%
  step_integer(all_nominal()) %>%
  prep(training = train_ml, retain = TRUE) %>%
  juice()
X_test <- as.matrix(xgb_test[setdiff(names(xgb_test), "log_price")])

predictions_xg <- predict(xgb.fit, X_test)

# Calculate RMSE 
rmse_xg <- sqrt(mean((test_ml$log_price - predictions_xg)^2)) 

# Print RMSE 
print(paste("RMSE for XG Boost:", rmse_xg))

# variable importance plot
# Compute feature importance matrix
importance_matrix = xgb.importance(model = xgb.fit)
importance_matrix
xgb.plot.importance(importance_matrix[1:7,])
features_xg = importance_matrix$Feature[1:7] # we select the top 7 features

formula <- as.formula(paste("log_price ~", paste(features_xg, collapse = " + ")))
xg_ols = lm(formula, data = test_ml)

# Make predictions on the test data
predictions_xgols <- predict(xg_ols, newdata = test_ml)
 
# Calculate RMSE
rmse_xgols <- sqrt(mean((test_ml$log_price - predictions_xgols)^2))
 
# Print RMSE
print(paste("RMSE for XGBoost OLS:", rmse_xgols))

summary(xg_ols)
```