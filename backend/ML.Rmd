---
title: "ML model"
author: "Jessica Widyawati"
date: "`r Sys.Date()`"
output: html_document
---

```{r warnings = FALSE}
#load the required libraries
library(tidyverse)
library(caret)
```

### Conduct stratified sampling and loading the train and test sets
```{r}
# Set seed for reproducibility
set.seed(42)

hdb_data = read.csv("processed_data/hdb_resale_prices.csv") %>% 
  select(-1)

# Stratified Sampling by 'year'
strata_cols <- c("year")

# Perform the stratified sampling for training set (60%)
train_data <- hdb_data %>%
  group_by(across(all_of(strata_cols))) %>%
  sample_frac(0.6) %>% # Adjust the fraction as needed for training set
  ungroup()

# Remaining data for validation and test sets
remaining_data <- anti_join(hdb_data, train_data)

# Split remaining data into validation and test sets (20% each)
validation_data <- remaining_data %>%
  group_by(across(all_of(strata_cols))) %>%
  sample_frac(0.5) %>% # Adjust the fraction as needed for validation set
  ungroup() 

test_data <- anti_join(remaining_data, validation_data)
```

## Create train,test and validation sets
```{r}
# from our EDA results, since we know that the distribution for the price variable is right skewed, we need to transform the price into log price to normalise

train_ml <- train_data %>% mutate(log_price = log(resale_price)) %>%
  select(-resale_price)

test_ml <- test_data %>% mutate(log_price = log(resale_price)) %>%
  select(-resale_price)

validation_ml <- validation_data %>% mutate(log_price = log(resale_price)) %>%
  select(-resale_price)
```

## Benchmark OLS model
```{r}
# Perform linear regression on all the covariates in the training data
model <- lm(log_price ~ ., data = train_ml)
 
# Make predictions on the test data
predictions <- predict(model, newdata = test_ml)
 
# Calculate RMSE
rmse_ols <- sqrt(mean((test_ml$log_price - predictions)^2))
 
# Print RMSE
print(paste("RMSE for benchmark OLS:", rmse_ols))

summary(model)
```

## Machine Learning Techniques
### Regression Trees: 
```{r}
# Load the package 
library(rpart) 

# Create decision tree using regression 
fit <- rpart(log_price ~ .,  
             method = "anova", data = train_ml) 
  
# Plot 
plot(fit, uniform = TRUE, 
          main = "log_price prediction using decision trees")
text(fit, use.n = TRUE, cex = .7) 

# Print model 
print(fit) 

# method anova is used for regression 
predictions_rt <- predict(fit, test_ml, method = "anova")

# Calculate RMSE
rmse_rt <- sqrt(mean((test_ml$log_price - predictions_rt)^2))
 
# Print RMSE
print(paste("RMSE for Regression Trees:", rmse_rt))
```

### Need changes
```{r}
library(randomForest)
#Now try the Random forest. To go from bagging to proper random forest, we #need to add the option mtry - number of predictors randomly sampled for each tree: #Here we set mtry=4 to reflect the default choice of P/3 for regression problems. 

rffit = randomForest(log_price~.,data=train_ml,ntree=50, mtry=4) 

plot(rffit) #plot the OOB error 

# method anova is used for regression 
predictions_rf <- predict(rffit, test_ml, method = "anova") 

# Calculate RMSE 
rmse_rt <- sqrt(mean((test_ml$log_price - predictions_rf)^2)) 

# Print RMSE 
print(paste("RMSE for Random Forest:", rmse_rt))
```

### Need changes
```{r}
# Gradient Boosting Machine (GBM)
# Load the gbm package
library(gbm)
library(caret)

# Separate predictors (X) and response variable (Y)
X <- subset(train_ml, select = -c(log_price)) 

Y <- train_ml$log_price

#tune number of trees and interaction depth using 10 fold CV


# Train GBM model, need to tune the n.trees and interaction.depth
gbm_model <- gbm(Y ~ ., data = X, distribution = "gaussian", n.trees = 100, interaction.depth = 3)

# Summary of the trained model
summary(gbm_model)

# Predictions on training data
predictions_gbm <- predict(gbm_model, X, n.trees = 100, type = "response")

# Evaluate model performance (if needed)
print(gbm_model)

# Calculate RMSE
rmse_gbm <- sqrt(mean((test_ml$log_price - predictions_gbm)^2))
 
# Print RMSE
print(paste("RMSE for Gradient Boosting Model:", rmse_gbm))
```

##XG Boost
```{r}

```

### Need changes
## Hedonic price function using post-LASSO and feature engineering
```{r}
library(tidymodels)
library(tidyverse)
library(mlbench)

#define pre-processing of data using the recipe package
rec <- recipe(log_price ~ ., data = train_ml)

lasso_mod <- linear_reg(mode = "regression",
                        penalty = tune(),
                        mixture = 1) %>% 
  set_engine("glmnet")

wf <- workflow() %>%
  add_model(lasso_mod) %>%
  add_recipe(rec) %>%
  fit(data = train_ml)

lasso_fit <- wf %>% pull_workflow_fit() %>%
  tidy()

#tuning using bootstrap
set.seed(1234)
office_boot <- bootstraps(validation_ml, strata = season)

tune_spec <- linear_reg(penalty = tune(), mixture = 1) %>%
  set_engine("glmnet")

lambda_grid <- grid_regular(penalty(), levels = 50)

#tune using grid
doParallel::registerDoParallel()

set.seed(2020)
lasso_grid <- tune_grid(
  wf %>% add_model(tune_spec),
  resamples = office_boot,
  grid = lambda_grid)

#plot performance results with the regularization parameter

lasso_grid %>%
  collect_metrics() %>%
  ggplot(aes(penalty, mean, color = .metric)) +
  geom_errorbar(aes(
    ymin = mean - std_err,
    ymax = mean + std_err),
  alpha = 0.5) +
  geom_line(size = 1.5) +
  facet_wrap(~.metric, scales = "free", nrow = 2) +
  scale_x_log10() +
  theme(legend.position = "none")

# pick lowest rmse and finalise lasso

lowest_rmse <- lasso_grid %>%
  select_best("rmse", maximize = FALSE)

final_lasso <- finalize_workflow(
  wf %>% add_model(tune_spec),
  lowest_rmse
)

#see the most important variables
library(vip)

final_lasso %>%
  fit(office_train) %>%
  pull_workflow_fit() %>%
  vi(lambda = lowest_rmse$penalty) %>%
  mutate(
    Importance = abs(Importance),
    Variable = fct_reorder(Variable, Importance)
  ) %>%
  ggplot(aes(x = Importance, y = Variable, fill = Sign)) +
  geom_col() +
  scale_x_continuous(expand = c(0, 0)) +
  labs(y = NULL)

#fit using test data
last_fit(
  final_lasso,
  test_ml
) %>%
  collect_metrics()
## # A tibble: 2 x 3
##   .metric .estimator .estimate
##   <chr>   <chr>          <dbl>
## 1 rmse    standard       0.381
## 2 rsq     standard       0.234

```

# Artificial Neural Network (ANN)
```{r}

ann_model <- train(
  log_price ~ ., 
  data = train_ml, 
  method = "nnet", 
  trControl = trainControl(method = "cv", number = 10), 
  tuneLength = 10,
  trace = FALSE # Set to TRUE for verbose output
)

```

# Principle Component Analysis
```{r}
# From our EDA, since we know that some variables are highly correlated with each other, we transform the variables into independent variables using PCA

# Preprocessing: Standardize the features
# pca is train and test sets?
scaled_X_train <- scale(train_ml)

# Perform PCA
pca_train <- prcomp(scaled_X_train)

# Extract transformed data
X_pca_train <- pca_train$x

# X_pca now contains your data transformed using PCA
```